{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PruneTreeByMergingCentroids:\n",
    "    __init__:\n",
    "        Parameters:\n",
    "            prefixString: It is a predefined string which will be used when generating cluster ids.\n",
    "            cltree: The base cltree on which the pruning mechanism will be applied.\n",
    "                    This has to be an instance of CLTree.\n",
    "            balancedPrune: It is a boolean flag, value of which indicates if we want to create a \"balanced\" clusters\n",
    "                           in terms of number of data points each cluster get.\n",
    "                \n",
    "        Operational Details:\n",
    "                Suppose we have divided the whole customer space into 100 small groups by a CLTree and \n",
    "                each group at most has 5 data-points (This means the underlying CLTree has 100 leaves and\n",
    "                each leaf at most has 5 data points). If we look at the customer data space graphically after\n",
    "                CLTree has performed, we should realize that the whole customer space is divided by hyper rectangles\n",
    "                and each rectangle is bounded by the cut which has been made to create that rectangle \n",
    "                (Classic decision tree and CLTree use the similar way to divide the whole data space gradually,\n",
    "                 so if you search internet how decision tree divides the whole data space into numerous hyper \n",
    "                 rectangles, you should be able to find a fair amount of quality material and same logic applies \n",
    "                 here in the case of CLTree). Let’s say, we have a requirement that each cluster has to encompass\n",
    "                at least 50 data points, otherwise it is too small to maintain. We have 100 small hyper-rectangles\n",
    "                and each has at-most 5 data points each, and now we have a job at hand that we need to combine these\n",
    "                100 hyper-rectangles gradually in a scientific way so that we can form groups of hyper-rectangles \n",
    "                where each group will be big enough to encompass at least 50 data points. \n",
    "\n",
    "                One of the better ways to represent a group of hyper-rectangles, when the goal is to measure how close\n",
    "                they are from others in the space respectively, is to represent a rectangle by it’s centroid. \n",
    "                \n",
    "                What is “centroid”?\n",
    "                Refer to here: https://en.wikipedia.org/wiki/Centroid\n",
    "\n",
    "                How to calculate centroid of a rectangle?\n",
    "                Refer to here: https://www.engineeringintro.com/mechanics-of-structures/centre-of-gravity/centroid-of-rectangle/\n",
    "\n",
    "                We have all the information needed to calculate centroids of all the hyper-rectangles. \n",
    "                How?\n",
    "                Well, all the leaves are hyper-rectangle. \n",
    "                The boundary line of each hyper-rectangle at each dimension is represented by all the min and max\n",
    "                of all the attributes of respectively of the underlying leaf. \n",
    "                Now, all there is left to do is calculation the centroid for each hyper rectangle represented by the\n",
    "                leaves of the CLTree. \n",
    "                Once we get all the representation vectors (centroids of all the hyper-rectangle) we will compare\n",
    "                the distance between one to other representation vectors and merge the pair together which are the \n",
    "                closest by the distance metric. After the merge of a pair, we recalculate the representation vector\n",
    "                of the pair by calculating weighted average of the two representation vector  and then replace the\n",
    "                individuals by the pair in the active participation group and continue the same process until our\n",
    "                condition is stratifying metric and quality metric is met.  \n",
    "\n",
    "\n",
    "    prune:\n",
    "        Parameters:\n",
    "            min_y: The \"min_y\" dictates the minimum number of data points required for a group to be \n",
    "                   considered as a cluster. This is an instance level variable. \n",
    "            searchMode: Boolean Flag, to indicate if the current pruning process, which is happending as part of some\n",
    "                        grid search mechanism to calculate the best possible value for an hyper-parameter or not. \n",
    "                        If the value of this flag is False that means, we have already decided what would the values\n",
    "                        of all the relevant hyper parameters and therefore have freedom to modify the structure \n",
    "                        of the underlying CLTree to optimize the search space.\n",
    "                        If the value of the flag is True, we do not change the structure of the underlying CLTree, so\n",
    "                        that we can reset all the modfications after calculating the quality metric at the end of the\n",
    "                        process and revert back all changes made to the underlying CLTree and get back the original one,\n",
    "                        which would be used for other values of the hyper parameter we are trying to optimize for.\n",
    "\n",
    "        Operational Details: This is the driver, only external facing and wrapper function of the class. The pruning\n",
    "                             process, whatever it may be, is controlled from here. \n",
    "                Algorithm:\n",
    "                    1. Validate and set the value of instance level variable \"min_y\".\n",
    "                    2. Initialize instance level variable \"maxSoFar\"(will be used to create unique cluster id)\n",
    "                       and \"version\".\n",
    "                    3. Initialize the some active instance level variables, totalDataInstancesbyEachGroup, \n",
    "                        finalRepresentationVectors, finalRepresentationVectorsToCluster, originalRepresentationVectors\n",
    "                    4. Invoke, an internal function \"intializeTouchedNodes\" to set the \"touching nodes\" at each leaf\n",
    "                       node level for the underlying CLTree. \n",
    "                    5. Invoke, an internal function \"initializeCentroids\" to get the original set of hyper-rectangle\n",
    "                        centroids by each leaf node. [If there are some \"touching nodes\", the combination \n",
    "                        a leaf nodes and with all it's \"touching nodes\" appear once.].\n",
    "                    6. Get the final \"totalDataInstancesbyEachGroup\", \"finalRepresentationVectors\",\n",
    "                       \"finalRepresentationVectorsToCluster\" by invoking \"pruningTreeByMergingRepresentationVectors\" with\n",
    "                       instance variable \"originalRepresentationVectors\" and \"min_y\". \n",
    "                    7. Measure the final cluster statistics by invoking \"clusterStatistics\" function with \n",
    "                       instance level \"originalRepresentationVectors\", \"finalRepresentationVectors\",\n",
    "                       \"finalRepresentationVectorsToCluster\", \"totalDataInstancesbyEachGroup\" and store it in \"result\". \n",
    "                    8. If we are not currently searching of an optimized value for an hyper parameter and had decided\n",
    "                       on the values of all the parameters, then we are free to \"prune\" the underlying CLTree to \n",
    "                       optimize the search space. The optimization of the search space for the cost of little\n",
    "                       modificantion of the underlying CLTree doesn't change the result or any details other than \n",
    "                       may be depth/hight of some nodes where applicable. The process is performed by invoking \n",
    "                       \"pruningRedundantNodes\" with the \"root\" of the uinderlying CLTree. \n",
    "                    9. Return the final result to the calling function.\n",
    "        \n",
    "    identifyingTouchedRegions:\n",
    "        Parameters:\n",
    "            root: An instance of CLNode which is root of a CLTree instance.\n",
    "                \n",
    "        Operational Details:\n",
    "            Defination:\n",
    "                **This operation is only valid for a CLTree.**\n",
    "                In a complex situation, it is possible that a broadly similar group of points got splited\n",
    "                into several regions either because the group is bounded by an irregular shape or because\n",
    "                in order to isolate one or more sparse region(s) from the original the underlying region,\n",
    "                it is cut into more than one piece.\n",
    "                \n",
    "                A region, Y1, is said to touch another region, Y2, on the i-th dimension on the lower bounding\n",
    "                surface (or the upper bounding surface), if they meet on the i-th dimension and intersect on all\n",
    "                other dimensions. That is,\n",
    "                1. max(Y1, i) = min(Y2, i) (or max(Y2, i) = min(Y1, i)) [for all i in cols./attrs. of data points]\n",
    "                2. And min(Y1, j) < max(Y2, j) and max(Y1, j) > min(Y2, j) [for all j ≠ i]\n",
    "                \n",
    "                **Algorithm**:\n",
    "                    1. Get the \"preordered\" list of leaves. [We are only concern about the leaves because at the end\n",
    "                                                          these are the only set of nodes which contains all the\n",
    "                                                          data points]\n",
    "                    2. Initialize \"mergeList\", a blank list which will hold the pair of leaves which are touching\n",
    "                                                            each-other by the above mentioned criterion.\n",
    "                    3. for node_preOrderedPos in range(len(PreOrderedList_of_leaves)):\n",
    "                        i. node = PreOrderedList_of_leaves[node_preOrderedPos]\n",
    "                        ii. get the min and max of all the attribute values of the node.\n",
    "                        iii. for all nxt_node in PreOrderedList_of_leaves[node_preOrderedPos:]:\n",
    "                            a. for each attr_i in attributes:\n",
    "                                **check for criteria 1. of the defination, if satisfies proceed to the step \"a)\"**\n",
    "                                    a) for each attr_j in attributes except attr_i:\n",
    "                                        **check for criteria 2. of the defination**\n",
    "                            b. if both criteria 1 and 2 are satisfied a pair of leaf node:\n",
    "                                a) Add the pair (node, nxt_node) to \"mergeList\".\n",
    "                            \n",
    "                            end for\n",
    "                        \n",
    "                        end for\n",
    "                        \n",
    "                    4. Declare an empty dictionary named \"touchingNodes\", which will contain standarized list of\n",
    "                            toching nodes for each node if a node is touching atleast one other node. \n",
    "                    5. if \"mergeList\" is not empty:\n",
    "                        a. invoke \"normalizingMergeList()\" with \"mergeList\",\n",
    "                                            it will return the content of \"touchingNodes\"\n",
    "                    6. Return \"touchingNodes\" to the calling function.\n",
    "                                \n",
    "                                    \n",
    "    \n",
    "    normalizingMergeList:\n",
    "        Parameters: \n",
    "            mergeList: list of pair of nodes which are touching each-other by defination of \"touching nodes\".\n",
    "                \n",
    "        Operational Details:\n",
    "            Create and return standarize list of touched nodes by each nodes respectively. This\n",
    "            function takes care of \"Commutative\" and \"Transitive\" property to create standarize list of\n",
    "            \"touching nodes\" for each node. \n",
    "            Ex.: mergeList: [(A,B), (C,D)] ==> touchingNodes: {A: [B], B: [A], C: [D], D:[C]}\n",
    "                 mergeList: [(A,B), (B,C)] ==> touchingNodes: {A: [B, C], B: [A, C], C: [A, B]}\n",
    "                        \n",
    "            returns touchingNodes to the calling function.\n",
    "            \n",
    "    \n",
    "    \n",
    "    calcModifiedDataLengthForEachnode:\n",
    "        Parameters: \n",
    "            node: An instance of CLNode (which is part of a CLTree instance).\n",
    "                \n",
    "        Operational Details:\n",
    "            This function initializes the \"modifiedLength\" property/attribute of CLNode.\n",
    "            After initializing the \"touching nodes\" for each node, the idea is the node itself and all the\n",
    "            touching node will **virtually** be considered a sigle node even though they are physically apart.\n",
    "            So if we want to know how many data points are there in any node at any given point of time, we\n",
    "            should include the length of data of \"touching nodes\" as well as virtually they are part of the\n",
    "            same node. \n",
    "            \n",
    "            This is a recursive function and operation on the CLTree is \"post-order\"[processing the children\n",
    "            first, then the node iself at any node] in nature.\n",
    "            \n",
    "            Algorithm:\n",
    "                1. if the node is Leaf:\n",
    "                    a. modfiedDataLength = own data length + ∑(data length of all \"touching nodes\")\n",
    "                    b. node.modifiedDataLength = modifiedDataLength\n",
    "                    c. return to the calling function\n",
    "                2. else:\n",
    "                    a. calculate the modifided data length of the left child.\n",
    "                    b. calculate the modifided data length of the right child.\n",
    "                    c. modfiedDataLength = left child modified data length + right child modified data length\n",
    "                    d. node.modifiedDataLength = modifiedDataLength\n",
    "                    e. return to the calling function\n",
    "                \n",
    "            This function set the values at the node level and doesn't return any value to the calling function.\n",
    "            \n",
    "    intializeTouchedNodes:\n",
    "        Parameters:\n",
    "            root: An instance of CLNode which is root of a CLTree instance.\n",
    "                \n",
    "        Operational Details:\n",
    "            This function is driver for getting a standrized list of \"touching nodes\" by each leaf nodes and set \n",
    "            the values for touchedNodes, modifiedLength at the CLNode instance level. \n",
    "            [Refer to \"identifyingTouchedRegions\" to get a clear idea about \"touching nodes\"]\n",
    "            Algorithm:\n",
    "                1. get the standrized list of \"touching nodes\" by each leaf nodes by invoking\n",
    "                                                                            \"identifyingTouchedRegions\".\n",
    "                2. Assign the list of \"touching nodes\" by each leaf nodes at CLNode instance level one by one. \n",
    "                3. calculate and initialize \"modifiedLength\" property/attribute of each CLNode by\n",
    "                                                             invoking \"calcModifiedDataLengthForEachnode\".\n",
    "            \n",
    "            This function set the values at the CLNode instance level and\n",
    "                                                doesn't return any value to the calling function.    \n",
    "        \n",
    "\n",
    "    _accountForTouchedNodesForCentroids:\n",
    "        Parameters:\n",
    "            preOrderedListofLeaves: A list of leaves which was generated by traversing the initial CLTree in preorder.\n",
    "            hyperRectangleCentroids: A python dictionary containing the initial centroid vector for each hyperrectangle\n",
    "                                     created by each leaf.\n",
    "            attributes: List of all the attributes in a constant order.\n",
    "                \n",
    "        Operational Details: This is an internal helper function which is invoked from _calcInitialCentroids\n",
    "                             to calculate the resultant hyper-rectangle centroids by combining the\n",
    "                             hyper-rectangle centroids of the \"touching nodes\" of each leaf and return the modified \n",
    "                             collection of hyper-rectangle centroids to the calling function.\n",
    "                             \n",
    "            Algorithm:\n",
    "                1. Declare an empty python dictionary to store and return the new set of hyper-rectangle centroids.\n",
    "                2. for each leaf in preOrderedListofLeaves:\n",
    "                    i. get the \"touching nodes\" of the current leaf. \n",
    "                    ii. if the current leaf node has any \"touching nodes\":\n",
    "                        a. add the current leaf node at the beginning of the list of \"touching nodes\".\n",
    "                        b. sort the current list of \"touching nodes\".\n",
    "                        c. if we are seeing the sorted current list of \"touching nodes\" first time:\n",
    "                            a. calculate the weighted average of all \"hyperRectangleCentroids\" of all the nodes\n",
    "                                                            in current list of \"touching nodes\".\n",
    "                            b. store the sorted current list of \"touching nodes\" as key and the result from previous\n",
    "                                        step as the value for future use to the python dictionary declared at step 1. \n",
    "                        d. if we have already seen the combination before, that mean this group is already accounted\n",
    "                                        for and we can now move to the next leaf in the preOrderedListofLeaves. \n",
    "                    iii. if the current leaf node doesn't have any \"touching nodes\", copy the current leaf node and\n",
    "                                        its associated centroid to the python dictionary declared at step 1.\n",
    "                3. end for\n",
    "                4. Return the new set of hyper-rectangle centroids to the calling function.\n",
    "            \n",
    "    _calcInitialCentroids:\n",
    "        Parameters:\n",
    "            preOrderedListofLeaves: A list of leaves which was generated by traversing the initial CLTree in preorder.\n",
    "            attributes: List of all the attributes in a constant order.\n",
    "                \n",
    "        Operational Details: This is an internal function with restricted access which is reposible for creating\n",
    "                             initial centroids of all the hyper-rectangle created by leaves of original CLTree. \n",
    "                             This function performs its operation by visiting each leaf node one by one from the \n",
    "                             \"preOrderedListofLeaves\" list and it first calculate the difference between max and\n",
    "                             min for each attribute one by one for each leaf, then divide all the amounts by 2 to \n",
    "                             get the mid point and add those to the respective min values to get the centroid of \n",
    "                             the hyper-rectangle created by the current leaf. After calculating the centroid of each\n",
    "                             leaf separately we need to account for \"touching nodes\", we do that by invoking another\n",
    "                             internal function \"_accountForTouchedNodesForCentroids\" with the initial list of\n",
    "                             centroids and get back a modified list of centroids which is returned to the calling\n",
    "                             function.\n",
    "                            \n",
    "                            Algorithm:\n",
    "                                1. Declare an empty python dictionary named \"hyperRectangleCentroids\". \n",
    "                                2. for each leaf in \"preOrderedListofLeaves\":\n",
    "                                    i. for each attribute in attributes:\n",
    "                                        a. Get the min value of the current attribute of the current leaf and store it. \n",
    "                                        b. Get the max value of the current attribute of the current leaf and store it.\n",
    "                                    ii. Get the vector difference between max-vector and min-vector. \n",
    "                                    iii. Calculate the centroid by adding the difference vector to the min vector. \n",
    "                                    iv. Store the calculated centroid against the leaf for further use and move-on. \n",
    "                                3. Invoke \"_accountForTouchedNodesForCentroids\" to account for \"touching nodes\" with\n",
    "                                   the initial list of centroids along with \"preOrderedListofLeaves\" and \"attributes\"\n",
    "                                   and get back a modified list of centroids which is returned to the calling function.\n",
    "                             \n",
    "    initializeCentroids:\n",
    "        Parameters: \n",
    "            root: Instance of CLNode which is the root of underlying CLTree.\n",
    "                \n",
    "        Operational Details: This function is responsible to calculate the initial and original list of centroids and\n",
    "                             return the result to the calling function. \n",
    "                             Algorithm:\n",
    "                                1. Validate the \"root\". \n",
    "                                2. Invoke \"preOrderTraversal\" with the root and empty list to get the list of leaves\n",
    "                                   which was generated by traversing the initial CLTree in preorder.\n",
    "                                3. Get the list of attributes from the root. \n",
    "                                4. Invoke \"_calcInitialCentroids\" with the pre-ordered leaves and the attributes we got at\n",
    "                                   step 2 and 3 respectively and get back a python dictionary which contain the initial\n",
    "                                   hyper-rectangle centroids before the pruning process begins and return it to the\n",
    "                                   calling function.\n",
    "\n",
    "\n",
    "    **Defination**: A unit vector is a vector of length 1, sometimes also called a direction vector.To find a\n",
    "                     unit vector with the same direction as a given vector, we divide by the magnitude of the vector. \n",
    "                                                                  \n",
    "    _getUnitVectorOfRepresentationVectors:\n",
    "        Parameters:\n",
    "            representationVectors: A python dictionary, values of which are vectors. \n",
    "                \n",
    "        Operational Details: This is an internal utility function with restricted access. Given a collection of vectors\n",
    "                             this function converts all the underlying vectors into \"unit vector\" individually and \n",
    "                             return a dictionary with the same set of keys and respective unit-vectors of previously\n",
    "                             provided vectors. \n",
    "        \n",
    "    _recalculateRepresentationVectors:\n",
    "        Parameters:\n",
    "            key1: tuple of node id(s)\n",
    "            representationVector1: Current representation vector of the collection of nodes provided as key1.\n",
    "            key2: tuple of node id(s)\n",
    "            representationVector2: Current representation vector of the collection of nodes provided as key2.\n",
    "                \n",
    "        Operational Details: This is an internal utility function with restricted access. Given 2 collection of \n",
    "                             node ids and their respective representation vectors this function calculates weighted\n",
    "                             average of 2 represetation vectors based on number of actual data instances each\n",
    "                             collection encompasses and create a combined key from \"key1\" and \"key2\" and return the\n",
    "                             newly calculated weighted average of two initial vectors as the associated value of the\n",
    "                             new key to the calling function. \n",
    "    \n",
    "    \n",
    "    _calcDataInstancesbyEachGroup:\n",
    "        Parameters:\n",
    "            listOfNodeIds: A list of tuples of node ids.\n",
    "                \n",
    "        Operational Details: This is an internal utility function with restricted access. Given a list of collection\n",
    "                             of leaf node ids, this function calculates and returns how many actual data instances \n",
    "                             are there by each collection of node ids. \n",
    "        \n",
    "\n",
    "    pruningTreeByMergingRepresentationVectors:\n",
    "        Parameters:\n",
    "            originalRepresentationVectors: A python dictionary containing the initial representation vectors for\n",
    "                                           each initial group of leaf nodes [post accounted for \"touching nodes\"]. \n",
    "                                           Here representation vectors are the centroids of the hyper rectangle created \n",
    "                                           by the leaf nodes[or weighted average of centroids of \"touching nodes\"].\n",
    "                            \n",
    "            min_y: The minimum number of members required by each group of leaf nodes to be considered as an\n",
    "                individual cluster.\n",
    "                \n",
    "        Operational Details:\n",
    "            Base Algorithm:\n",
    "            Agglomerative Hierarchical Clustering: It is a type of clustering algorithm which starts by treating\n",
    "                each object as a singleton cluster. Next, pairs of clusters are successively merged until all\n",
    "                clusters have been merged into one big cluster containing all objects. The result is a\n",
    "                tree-based representation of the objects, named dendrogram.\n",
    "\n",
    "            Quality Metric/ argmin Metric:\n",
    "            2 different types Quality Metric or argmin Metric has been used in this implementation of the algorithm.\n",
    "            The value of the instance level flag \"balancedPrune\". If True, then it indicates we care about the \n",
    "            variations among the number of cluster members of clusters. If False, then it indicate we care only about\n",
    "            the quality of clusters. \n",
    "            \n",
    "            Purity or Inv-Purity: intra-clusters-distance / inter-clusters-distance\n",
    "            intra-cluster-distance and inter-cluster-distance are two very common quality metric is used to\n",
    "            measure the quality of clusters or performance of the clustering algorithm. We always want to \n",
    "            minimize intra-cluster-distance and maximize inter-clusters-distance for a given set of clusters.\n",
    "            Here we have combined 2 term together and as we always want to minimize the numerator and maximize\n",
    "            the denominator, our goal would be to minimize the value of our “Quality Metric”.\n",
    "            \n",
    "            varPurity/varInv-Purity: Standard deviation of cluster members of clusters ^ Purity or Inv-Purity\n",
    "            As we have already established that why we want to minimize \"Purity or Inv-Purity\". At the \"balancedPrune\"\n",
    "            mode we have an additional constraint which is variance/standard-deviation of number data points among\n",
    "            clusters. We want to decrease \"Purity or Inv-Purity\" and \"variance or standard-deviation\",. By combining\n",
    "            these two, we have created a new metric which gives emphasis on both metric at the same time. At the\n",
    "            begining of an experiment when do not know anything or not have enough confidence it is recommended\n",
    "            to do balanced clustering. \n",
    "            \n",
    "\n",
    "            Defense/Protection against outliers:\n",
    "            In clustering there is a common occurrence of a problem that there may be few data points scattered across\n",
    "            the hyper-plane which are not close to any well-formed groups. Now the question is how to handle it?\n",
    "            Here in our clustering scheme, we have an instance level parameter “min_y” which indirectly controls\n",
    "            the number of clusters by defining how many minimum data points we would need to identify a group as a\n",
    "            well-formed cluster. We don’t want to violate the integrity of well formed clusters to include some\n",
    "            outliers here and there, so one of the better way of handling this kind of scenario is form a separate\n",
    "            group of outliers by the name of “DEFAULT”, in this way we are protecting integrity of the well-formed\n",
    "            clusters and also creating mutually exclusive and exhaustive clusters. \n",
    "            We initiate the above-mentioned mechanism of “Protection against outliers” when the number of points\n",
    "            which are not part of a well-formed group is less than the instance level parameter “min_y”. \n",
    "            This way we are eliminating any possibility that they might form a well-formed cluster by merging\n",
    "            one to another. \n",
    "\n",
    "\n",
    "            This is the heart of the “PruneTreeByMergingCentroids”. At this point, our entire data space is\n",
    "            divided into small groups, we have their representation vectors and another parameter, “min_y”.\n",
    "            We will use the representation vectors as the source data to a hierarchical agglomerative clustering\n",
    "            to group together small groups to form bigger groups. We will keep merging the small groups to form\n",
    "            bigger groups until either all the remaining groups have “min_y” data points or total data points\n",
    "            left to be part of bigger groups is less than “min_y”. In case of the later, we temporarily bind all\n",
    "            the loose groups/data points which are not part of any qualified group and form a group.\n",
    "            Then we calculate the statistics of the current settings and store it for future reference.\n",
    "            Then we continue with the hierarchical agglomerative clustering until there is only 2 groups\n",
    "            remaining but from now on after each iteration, we take a snapshot of the settings and the statistics\n",
    "            for future reference. \n",
    "            At the end we take all the scenarios, calculate the quality metric and use gradient tolerance to\n",
    "            find out the best setting and return the best way to cluster to the calling function. \n",
    "\n",
    "            Algorithm:\n",
    "                1. Make a \"deep copy\" of initial representation vectors. \n",
    "                2. calculate number of data points at each small groups and make a \"deep copy\" of the result.\n",
    "                3. Get all the representation vectors in a way so that it can be directly used my the\n",
    "                   \"scipy.cluster.hierarchy\" for hirerchical clustering and also, create referece of the\n",
    "                    data points by using keys from the copy of the \"originalRepresentationVectors\" created at step 1.\n",
    "                    So that later when we will revisit the hirerarchical cluster tree from the bottom up manner\n",
    "                    we can referce the original CLTree nodes from there. \n",
    "                4. Min-Max normalization of the data which will be used for hierarchical clustering to remove any\n",
    "                   potential bias. \n",
    "                5. Hierarchical clustering on the normalized data. \n",
    "                6. declare an empty dictionary to store all the intermediate results as we are about to revisit the \n",
    "                   hierarchical cluster tree from bottom-up manner. \n",
    "                7. While there are more than 2 groups:\n",
    "                    i. get the 2 vertices got combined. \n",
    "                    ii. get the actual node ids(or tuple of node ids) from the refernce has been created at step 3. \n",
    "                    iii. Create a new new combination from the 2 tuple we got back at previous step. It will be used\n",
    "                        at id in to identify the combination of nodes.\n",
    "                    iv. create a new refernce with the newly created id and delete previous references. \n",
    "                    v. Check if all the groups at this moment has data points more or equal to \"min_y\". \n",
    "                        a. if so, take a snapshot of the current groups and move on to next iteration. \n",
    "                    vi. Otherwise, get 2 lists of groups which have atleast \"min_y\" datapoints and groups which do not\n",
    "                        respectively. \n",
    "                        a. Check if total members of the groups which do not have atleast \"min_y\" datapoints is\n",
    "                           less than \"min_y\". \n",
    "                            i> if so, combine all the non-qualified groups together to form a temporary default group.\n",
    "                            ii> take a snapshot of the current groups and move on to next iteration.\n",
    "                        b. Otherwise move-on to the next iteration. \n",
    "                        c. end if\n",
    "                    viii. end if\n",
    "                8. end while\n",
    "                9. Validation for no cluster found, it can happen due to bad value in parameters. \n",
    "                10. Now it is the time to revisit all the snapshots we have taken during our visit to dendogram\n",
    "                    to figure out the best possible setting by the chosen metric.\n",
    "                    **[argminMetric = \"purity\" or \"varPurity\", depending on the value of \"balancedPrune\".]**\n",
    "                11. Once we have finalized a setting, we assign an unique cluster id to each group in the\n",
    "                    finalized settings and also as a part of this process all the leaf nodes get assigned \n",
    "                    to a cluster id based on which group they belong to.\n",
    "                12. Rerturn the best setting, final representation vectors and a map from node ids to cluster ids to the\n",
    "                    calling function.        \n",
    "\n",
    "\n",
    "    _assigingClusterToInternalNodes:\n",
    "        Parameters:\n",
    "            scenario: Finalized cluster settings in python dictionary format. Where the keys are tuples made-up with\n",
    "                     CLTree node ids. \n",
    "                \n",
    "        Operational Details:\n",
    "            After finalizing the clusters and the leaf nodes each one encompasses, it is time to assign an unique \n",
    "            cluster id to each cluster. In addition to this, the purpose of this function is to set the cluster\n",
    "            flag of each leaf node one by one and assign the cluster-id to the node attribute based on which cluster \n",
    "            the underlying leaf node is part of. At the same time create a refernce from node ids to assigned\n",
    "            cluster ids and return the same to the calling function.\n",
    "    \n",
    "    **Defination**: In computer science, tree traversal (also known as tree search) is a form of graph traversal\n",
    "                    and refers to the process of visiting (checking and/or updating) each node in a tree data\n",
    "                    structure, exactly once. Such traversals are classified by the order in which the nodes are\n",
    "                    visited.\n",
    "                A Generic Pre-Order Traversal Algorithm:\n",
    "                    1. Check if the current node is empty or null.\n",
    "                    2. Display/Store the data part of the root (or current node).\n",
    "                    3. Traverse the left subtree by recursively calling the pre-order function.\n",
    "                    4. Traverse the right subtree by recursively calling the pre-order function.\n",
    "                                                                  \n",
    "                                                                  \n",
    "    preOrderTraversal:\n",
    "        Parameters:\n",
    "            node: Instance of CLNode class. \n",
    "            preOrderedList: Current list of visted leaf nodes of undelying CLTree.\n",
    "                \n",
    "        Operational Details: This is an internal utility recursive function which returns the pre-order list of\n",
    "                             visited **leaf nodes**. Unlike generic \"Pre-Order Traversal Algorithm\" this version\n",
    "                             doesn't return list of all the nodes. It visits all the nodes but returns only the leaf\n",
    "                             nodes in the order of traversal. \n",
    "                             \n",
    "                             Algorithm:\n",
    "                                    1. If the current node is a leaf node add the current node to the current list\n",
    "                                         of visted leaf nodes and return the current list.\n",
    "                                    2. If the current node is not a leaf node, get its children. \n",
    "                                         i. Invoke another copy of \"preOrderTraversal\" with the left child and current\n",
    "                                            list of visited leaf nodes and get back the modified current list of\n",
    "                                            visited leaf nodes. \n",
    "                                        ii. Invoke another copy of \"preOrderTraversal\" with the right child and current\n",
    "                                            list of visited leaf nodes and get back the modified current list of\n",
    "                                            visited leaf nodes.\n",
    "                                        iii. Return the modified current list of visited leaf nodes to the \n",
    "                                              calling function.\n",
    "                                \n",
    "            \n",
    "    preOrderTraversalofClusters:\n",
    "        Parameters:\n",
    "            node: Instance of CLNode class. \n",
    "            preOrderedList: Current list of visted leaf nodes of undelying CLTree which are part of some cluster.\n",
    "                \n",
    "        Operational Details: This is an internal utility recursive function which returns the pre-order list of\n",
    "                            visited **leaf nodes which are part of some cluster**. Unlike generic \n",
    "                            \"Pre-Order Traversal Algorithm\" this version doesn't return list of all the nodes. \n",
    "                            It visits all the nodes but returns only the leaf which are part of some cluster\n",
    "                            nodes in the order of traversal. \n",
    "                             \n",
    "                            Algorithm:\n",
    "                                      1. If the current node is part of some cluster add the current node to the\n",
    "                                         current list of visted \"includedInCluster\" nodes and return the current list.\n",
    "                                      2. If the current node is not \"includedInCluster\", then get its children. \n",
    "                                         i. If it is a leaf node, just return the unmodified current list of visited\n",
    "                                            leaf nodes which are part of some cluster.\n",
    "                                        ii. Invoke another copy of \"preOrderTraversalofClusters\" with the left child\n",
    "                                             and current list of visited \"includedInCluster\" nodes and get back the \n",
    "                                             modified current list of visited \"includedInCluster\" nodes. \n",
    "                                        iii. Invoke another copy of \"preOrderTraversal\" with the right child and \n",
    "                                              current list of visited \"includedInCluster\" nodes and get back the \n",
    "                                              modified current list of visited \"includedInCluster\" nodes.\n",
    "                                        iv. Return the modified current list of visited \"includedInCluster\" nodes to\n",
    "                                             the calling function.\n",
    "    \n",
    "    leafPrunning:\n",
    "        Parameters: \n",
    "            node: Instance of CLNode class. \n",
    "                \n",
    "        Operational Details: This is an internal utility recursive function which operate in a bottom-up fashion. \n",
    "                             At each level/node this function checks if 2 childern of a parent are part of the same\n",
    "                             cluster, if so we can safely assign the cluster at the parent level and by doing so, \n",
    "                             we are optimizing the search space by reducing it by one level at a time. \n",
    "    \n",
    "    _whichChild:\n",
    "        Parameters:\n",
    "            node: Instance of CLNode class. \n",
    "                \n",
    "        Operational Details: This is an internal utility function with restricted access, which, given a node,\n",
    "                             return a flag to indicate if the current node is left or right child of its parent. \n",
    "        \n",
    "\n",
    "    resetAllPruneNodes:\n",
    "        Parameters: \n",
    "            node: Instance of CLNode class.\n",
    "                \n",
    "        Operational Details: This is an utility recursive function which resets all the changes has been made\n",
    "                             at node level of the underlying CLTree as part of the current pruning process and revert\n",
    "                             back to its original structure.This funtion is called when doing grid search to find\n",
    "                             optimal values for the set of hyperparameters.\n",
    "        \n",
    "\n",
    "    _calcClusterMemebersVariance:\n",
    "        Parameters:\n",
    "            totalDataInstancesbyEachGroup: A python dictionary which contains data instances encompassed by each group\n",
    "                                           of leaf nodes.\n",
    "                \n",
    "        Operational Details: This is an internal utility function with restricted access which is called from\n",
    "                             \"clusterStatistics\". This function returns the mean of the number of data points \n",
    "                             accross the clusters and the variance among the number of data points accross the clusters\n",
    "\n",
    "    clusterStatistics:\n",
    "        Parameters:\n",
    "            originalRepresentationVectors: Python dictionary consist of original representation vectors. Keys are the \n",
    "                                           original leaf nodes after \"touching nodes\" are accounted for and values\n",
    "                                           are respective representation vectors before\n",
    "                                           \"pruningTreeByMergingRepresentationVectors\" was invoked. \n",
    "            finalRepresentationVectors: Python dictionary consist of final representation vectors. Keys are the final\n",
    "                                        groups of leaf nodes in tuple format which together forms invidual clusters \n",
    "                                        and values are respective representation vectors after\n",
    "                                        \"pruningTreeByMergingRepresentationVectors\" operation is completed. \n",
    "            finalRepresentationVectorsToCluster: Python dictionary consist of mapping from collection of leaf nodes \n",
    "                                                 to assigned respective cluster ids.\n",
    "            totalDataInstancesbyEachGroup: Python dictionary consist of mapping from group of leaf nodes and total\n",
    "                                           number of data instances by each group.\n",
    "                                           \n",
    "                \n",
    "        Operational Details:\n",
    "            This function is reposible to calculate and return the key statistics including the quality metric of choice \n",
    "            which will be used to asses the quality of clustering. In this particular implementation the quality metrics\n",
    "            are, purity or inv-purity, Formula: intraClusterDistance/interClusterDistance and varPurity or var-invPurity,\n",
    "            Formula: Standard-deviation among the number of data points at each cluster ^ purity or inv-purity respectively \n",
    "            Apart from the quality metric this function also returns the \"intra-cluster-distance\",\n",
    "            \"inter-cluster-distance\", \"mean-members\" accross the clusters  and \"data-points\" at each cluster under\n",
    "            the provided setting. \n",
    "            \n",
    "            Algorithm:\n",
    "                1. If the instance level variable \"balancedPrune\" is True set the argmin metric as \"varPurity\" else\n",
    "                   set the argmin metric as \"purity\". \n",
    "                2. Get the unit vector of each original and final representation vectors. \n",
    "                3. Create a map/dictionary for each final individual group of leaf nodes which was created \n",
    "                   by merging a subset of original set of leaf nodes. It would be useful at the time of calulating \n",
    "                   \"intra-cluster-distance\".\n",
    "                4. Create all possible pair of cluters. It would be useful at the time of\n",
    "                   calulating \"inter-cluster-distance\".\n",
    "                5. Invoke an internal function named, \"_calcClusterStatistics\" to get the \"inter-cluster-distance\"  and\n",
    "                   \"intra-cluster-distance\".\n",
    "                6. calculate the purity or inv-purity by \"intra-cluster-distance\"/\"inter-cluster-distance\".\n",
    "                7. Invoke \"_calcClusterMemebersVariance\" to get the mean of the number of data points accross the\n",
    "                   clusters and the variance among the number of data points accross the clusters.\n",
    "                8. Calculate \"varPurity\" using the information at step 7 and 6.\n",
    "                9. Put all the statistics together in \"result\".\n",
    "                10. Add \"argminMetric\" in the \"result\", according to the value of argminMetric decided at step 1. \n",
    "                11. Return the result to the calling function.\n",
    "        \n",
    "\n",
    "    _calcClusterStatistics:\n",
    "        Parameters:\n",
    "            allCombosOfFinalRepresentationVectors: All possible pair of cluters keys. It would be useful at the time of\n",
    "                                                   calulating \"inter-cluster-distance\"\n",
    "            finalRepresentationVectors: Python dictionary consist of final representation vectors. Keys are the final\n",
    "                                        groups of leaf nodes in tuple format which together forms invidual clusters \n",
    "                                        and values are respective representation vectors after\n",
    "                                        \"pruningTreeByMergingRepresentationVectors\" operation is completed.\n",
    "            originalRepresentationVectors: Python dictionary consist of original representation vectors. Keys are the \n",
    "                                            original leaf nodes after \"touching nodes\" are accounted for and values\n",
    "                                            are respective representation vectors before\n",
    "                                            \"pruningTreeByMergingRepresentationVectors\" was invoked.\n",
    "            finalToOriginalRepresentationVectorsMap: A python dictionary for each final individual group of leaf nodes\n",
    "                                                     which was created by merging a subset of original set of\n",
    "                                                     leaf nodes. It would be useful at the time of calulating \n",
    "                                                     \"intra-cluster-distance\".\n",
    "                \n",
    "        Operational Details: \n",
    "            This is an internal function which is called from \"clusterStatistics\", is responsible to \n",
    "             do the actual calculations required to calculate various cluster statistics including\n",
    "             \"intra-cluster-distance\" and \"inter-cluster-distance\" and return those to the calling function.\n",
    "            \n",
    "\n",
    "\n",
    "    _calcEuclidianDistance:\n",
    "        Parameters:\n",
    "            a: vector 1.\n",
    "            b: vector 2.\n",
    "                \n",
    "        Operational Details:\n",
    "            This is an utility function which calculates and returns the euclidian distance bewteen 2 vectors. \n",
    "    \n",
    "    getClusterID:      \n",
    "        Operational Details:\n",
    "            This function increment and get the current value of an instance level variable and then concatenate\n",
    "            the value with some predefined string to create and the return an unique id for a cluster.\n",
    "    \n",
    "    pruningRedundantNodes:\n",
    "        Parameters:\n",
    "            root: The root of the instance level CLTree.\n",
    "                \n",
    "        Operational Details:\n",
    "            This is an utility function. Purpose of this function is to make the search space of CLTree more efficient\n",
    "            after \"leafPrunning\" has happened. At core CLTree devides the dataset at any level like a binary search tree.\n",
    "            Suppose 2 leaf nodes are 1 generation apart, both are the same type of child of their respective parent and\n",
    "            at both respective parent level the cut has happend on the same attribute, then we can safely remove \n",
    "            one level from the existing CLTree. \n",
    "            **Note**: This method changes the structure of the core CLTree, so after this method is invoked the\n",
    "                      underlying CLTree can't be reused in grid-search or restored back to its original form.\n",
    "                      This method/function therefore should be only called during the final and permanent pruning\n",
    "                      process. \n",
    "            Each leaf node in CLTree holds an unique set of data, as we are removing one level, we would need to merge\n",
    "            the data of respective leaf node to the data set of the other leaf node which will represent the both leaves\n",
    "            together. \n",
    "            This is a bottom-up iterative procedure. \n",
    "        \n",
    "    _recalculateDepthOfEachNodes:\n",
    "        Parameters:\n",
    "            node: CLNode instance. Started with CLTree root. \n",
    "            depth: Current Depth of the node. Started woth 0.\n",
    "                \n",
    "        Operational Details:\n",
    "            This is a recursive procedure, the purpose of the procedure is recalculate the depth of all the nodes\n",
    "            of the CLTree after it has gone through some structural changes. It updates the depth at the node level.\n",
    "        \n",
    "    _merging2Datasets:\n",
    "        Parameters:\n",
    "            dataset1: Instance of CLTreeModules.Data class associated with one CLNode instance.\n",
    "            dataset2: Instance of CLTreeModules.Data class associated with another CLNode instance.\n",
    "                \n",
    "        Operational Details:\n",
    "            This is an internal function called from \"pruningRedundantNodes\" to merge dataset of 2 nodes together when \n",
    "            reducing a level. Some basic validation is requiered before the merge. Ex.: Order of the columns\n",
    "            name of all the columns and data-types of all the columns need to be same among 2 participating dataset. \n",
    "            If all the validations are fruitful then we create a separate instance of CLTreeModules.Data class \n",
    "            by using the combined dataset of dataset1 and dataset2 and return the newly created instance back to\n",
    "            calling function.\n",
    "\n",
    "pruneByGridSearch_Centroid:\n",
    "    Parameters:\n",
    "        cltree: The original CLTree instance on which the whole puning mechanism will be performed.\n",
    "        min_y: The \"min_y\" dictates the minimum number of data points required for a group to be considered as a cluster.\n",
    "        data_length: Total number of the data points on which the clustering is happening.\n",
    "        prefixString: predefined string for cluster id.\n",
    "        balancedPrune: It is a boolean flag, value of which indicates if we want to create a \"balanced\" clusters\n",
    "                       in terms of number of data points each cluster get.\n",
    "            \n",
    "    Operational Details: This is method to grid search on various parameter values required for creating cluster \n",
    "                         by pruning the initial CLTree or merging the leaf nodes to form clusters.\n",
    "                         In this implementation, \"min_y\" is the only manually provided value and also a \n",
    "                         hyper parameter. To get the most optimal value of \"min_y\" which will provide the \n",
    "                         best value for the cost function, we do a grid search on 10 equally spaced values of \n",
    "                         \"min_y\" starting from \"min_y\" - 10% of \"min_y\" to \"min_y\". At each value we create a \n",
    "                         instance of \"PruneTreeByMergingCentroids\" with the original \"cltree\" and current value of \n",
    "                         \"min_y\". At the end of each pruning procedure, we store the current\n",
    "                         qulaity metric with the current setting for future refernce. (Here in this implementation\n",
    "                         the cost/quality metric is \"purity\"[actually inverse purity.] or \n",
    "                         \"varPurity\" [actually std. ^ inverse purity.], which is we are trying to minimize.)\n",
    "                         Then move to the new value of \"min_y\". At the end when we are done with testing all the \n",
    "                         potential values of \"min_y\". We take the value of \"min_y\" where the quality metric is best\n",
    "                         to the final pruning with \"searchMode\" set to False. We set the base version of the pruning\n",
    "                         algorithm and return the base version and the result to the calling function. After the final\n",
    "                         pruning all the required changes has happened to the CLTree and CLNode level. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:AI-ML]",
   "language": "python",
   "name": "conda-env-AI-ML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
